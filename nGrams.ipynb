{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "\n",
    "The goal here is to implement a simple n-gram model for text generation. The model will be trained on a given text and then used to generate new text. We will also use a perplexity metric to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import math\n",
    "\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Document N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_ngrams(tokenised_document, n, min_frequency, ignore_case=True):\n",
    "    '''Find n-grams in a tokenised document.\n",
    "    \n",
    "    tokenised_document: A list of string tokens.\n",
    "    n: The n in n-grams.\n",
    "    min_frequency: The minimum frequency of an n-gram to be outputted.\n",
    "    ignore_case: If True, n-grams are case-insensitive.\n",
    "    returns: A list of tuples: n-gram, n-gram frequency.'''\n",
    "\n",
    "    if ignore_case:\n",
    "        tokenised_document = [word.lower() for word in tokenised_document]\n",
    "    ngrams = {}\n",
    "    for token_index in range(len(tokenised_document) - n + 1):\n",
    "        ngram = tuple(tokenised_document[token_index:token_index + n])\n",
    "        if ngram in ngrams:\n",
    "            ngrams[ngram] += 1\n",
    "        else:\n",
    "            ngrams[ngram] = 1\n",
    "    ngrams_subset = sorted([k for k, v in ngrams.items() if v >= min_frequency], key=ngrams.get, reverse=True)\n",
    "\n",
    "    return [(ngram, ngrams[ngram]) for ngram in ngrams_subset]\n",
    "\n",
    "def find_all_ngrams(tokenised_document, max_n, ignore_case=True):\n",
    "    '''Find all n-grams in a tokenised document, up to a maximum n.\n",
    "    \n",
    "    tokenised_document: A list of string tokens.\n",
    "    max_n: The maximum n in n-grams.\n",
    "    ignore_case: If True, n-grams are case-insensitive.\n",
    "    returns: A list per n, each with a list of tuples: n-gram, n-gram frequency.'''\n",
    "\n",
    "    all_ngrams = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ngrams = _find_ngrams(tokenised_document, n, min_frequency=1, ignore_case=ignore_case)\n",
    "        all_ngrams.append(ngrams)\n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating n-grams can be slow, so we persist the results, for decoupled analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_store_all_ngrams(filename, max_n):\n",
    "    print('Calculating all n-grams for ' + filename)\n",
    "    with open(filename, 'r', encoding=\"mbcs\") as file:\n",
    "        document = file.read()\n",
    "        tokenised_document = document.split()\n",
    "        all_ngrams = find_all_ngrams(tokenised_document, max_n)\n",
    "        ngram_counts = [len(ngram) for ngram in all_ngrams]\n",
    "        print(f'N-Gram counts: {ngram_counts}')\n",
    "        for n_index, ngrams in enumerate(all_ngrams):\n",
    "            with open(f'{filename}.{n_index + 1}.ngrams', 'w') as file:\n",
    "                for ngram, frequency in ngrams:\n",
    "                    file.write(str(ngram) + ',' + str(frequency) + '\\n')\n",
    "\n",
    "calculate_and_store_all_ngrams('./datasets/Alice.txt', max_n=10)\n",
    "calculate_and_store_all_ngrams('./datasets/Shakespeare.txt', max_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the n-grams from disk. We prefer to always load from disk, rather than from memory, as a simple way to guarantee consistency of the in memory data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_ngrams(root_filename, max_n):\n",
    "    print(f'Loading all n-grams up to n={max_n} for {root_filename}')\n",
    "    all_ngrams = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        print(f'Loading {root_filename}.{n}.ngrams')\n",
    "        with open(f'{root_filename}.{n}.ngrams', 'r') as file:\n",
    "            all_ngrams.append([eval(line.strip()) for line in file])\n",
    "    ngrams_counts = [len(ngrams) for ngrams in all_ngrams]\n",
    "    print(f'N-Grams\\' counts: {ngrams_counts}')\n",
    "    return all_ngrams\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_alice_ngrams = load_all_ngrams('./datasets/Alice.txt', max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_shakespeare_ngrams = load_all_ngrams('./datasets/Shakespeare.txt', max_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text from n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First preprocess the ngrams ready for usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_grouped_by_prior_tokens(all_ngrams):\n",
    "    return [_group_by_prior_tokens(ngrams) for ngrams in all_ngrams]\n",
    "\n",
    "def get_full_vocabulary(all_ngrams):\n",
    "    return list(set([token for ngrams in all_ngrams for tokens, _ in ngrams for token in tokens]))\n",
    "\n",
    "def _group_by_prior_tokens(ngrams):\n",
    "    grouped_tokens = {}\n",
    "    for token, frequency in ngrams:\n",
    "        prior_tokens = token[:-1]\n",
    "        final_token = token[-1]\n",
    "        if prior_tokens in grouped_tokens:\n",
    "            grouped_tokens[prior_tokens].append((final_token, frequency))\n",
    "        else:\n",
    "            grouped_tokens[prior_tokens] = [(final_token, frequency)]\n",
    "    return grouped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_vocabulary = get_full_vocabulary(all_alice_ngrams)\n",
    "alice_ngrams_grouped_by_prior_tokens = ngrams_grouped_by_prior_tokens(all_alice_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_vocabulary = get_full_vocabulary(all_shakespeare_ngrams)\n",
    "shakespeare_ngrams_grouped_by_prior_tokens = ngrams_grouped_by_prior_tokens(all_shakespeare_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input token sequence we build probabilities for all possible next tokens.\n",
    "\n",
    "Note: This implementation doesn't include handling of 'out of vocabulary' words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_probabilities(vocabulary, tokenised_input, ngrams_grouped_by_prior_tokens, max_n, laplace_constant=0.0001):\n",
    "    '''Get the probability of each token in the vocabulary given the input and n-grams.\n",
    "    \n",
    "    vocabulary: A list of all tokens in the inputs and outputs.\n",
    "    tokenised_input: A list of tokens.\n",
    "    ngrams_grouped_by_prior_tokens: A list of dictionaries, each with n-gram prior tokens as keys and a list of tuples: n-gram final token, n-gram frequency.\n",
    "    max_n: The maximum n in n-grams.\n",
    "    laplace_constant: A constant to add to the probability of each token to avoid zero probabilities.'''\n",
    "    \n",
    "    vocabulary_size = len(vocabulary)\n",
    "    weight_per_n_function = lambda n: n\n",
    "    n_values = range(1, max_n+1)\n",
    "    total_ngram_length_weights = sum([weight_per_n_function(n) for n in n_values])\n",
    "    ngram_length_weights_normalised = {n: (weight_per_n_function(n) / total_ngram_length_weights) for n in n_values}\n",
    "    unigram_additive_probability = (ngram_length_weights_normalised[1] / vocabulary_size) * laplace_constant\n",
    "    token_probabilities = {token: unigram_additive_probability for token in vocabulary}\n",
    "    for n in reversed(n_values[1:]):\n",
    "        _add_probablities_from_ngrams(tokenised_input, ngrams_grouped_by_prior_tokens[n - 1], ngram_length_weights_normalised[n], token_probabilities, n)\n",
    "    return token_probabilities\n",
    "\n",
    "def _add_probablities_from_ngrams(tokenised_input, ngrams_grouped_by_prior_tokens, ngram_length_weight, token_probabilities, n):\n",
    "    ngram_to_find = tuple(tokenised_input[-n+1:])\n",
    "    non_zero_frequencies = (ngrams_grouped_by_prior_tokens[ngram_to_find] if ngram_to_find in ngrams_grouped_by_prior_tokens else [])\n",
    "    additive_probabilities = _get_ngram_additive_probabilities(ngram_length_weight, non_zero_frequencies)\n",
    "    for token in additive_probabilities:\n",
    "        token_probabilities[token] += additive_probabilities[token]\n",
    "\n",
    "def _get_ngram_additive_probabilities(ngram_weight, non_zero_frequencies):\n",
    "    if not non_zero_frequencies:\n",
    "        return {}\n",
    "    total_frequencies = sum([frequency for _, frequency in non_zero_frequencies])\n",
    "    frequency_unit_probability = ngram_weight / total_frequencies\n",
    "    additive_probabilities = {token: frequency*frequency_unit_probability for token, frequency in non_zero_frequencies}\n",
    "    return additive_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sentence(input_string, vocabulary, ngrams_grouped_by_prior_tokens, max_length=100):\n",
    "    '''Generate and print a sentence given an input string, vocabulary, and n-grams.\n",
    "    \n",
    "    input_string: A string of space-separated tokens.\n",
    "    vocabulary: A list of all tokens in the inputs and outputs.\n",
    "    ngrams_grouped_by_prior_tokens: A list of dictionaries, each with n-gram prior tokens as keys and a list of tuples: n-gram final token, n-gram frequency.\n",
    "    max_length: The maximum length of the sentence to generate.'''\n",
    "    \n",
    "    print(f'Generating sentence for input: {input_string}')\n",
    "    tokenised_input = input_string.split()\n",
    "    max_n = len(ngrams_grouped_by_prior_tokens)\n",
    "    for _ in range(max_length):\n",
    "        token_probabilities = get_vocabulary_probabilities(vocabulary, tokenised_input, ngrams_grouped_by_prior_tokens, max_n)\n",
    "        next_token = _generate_token_from_token_probabilities(token_probabilities)\n",
    "        tokenised_input.append(next_token)\n",
    "    print(' '.join(tokenised_input))\n",
    "\n",
    "def _generate_token_from_token_probabilities(token_probabilities):\n",
    "    tokens = list(token_probabilities.keys())\n",
    "    token_weights = [token_probabilities[token] for token in tokens]\n",
    "    random_token = random.choices(tokens, weights=token_weights)[0]\n",
    "    return random_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sentence for input: Alice was\n",
      "Alice was reading, but it did not appear, and after a minute or two she walked on in the direction in which the march hare was said to live. â€œiâ€™ve seen hatters before,â€ she said to herself; â€œthe march hare will be much the most interesting, and perhaps as this is may\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sentence('Alice was', alice_vocabulary, alice_ngrams_grouped_by_prior_tokens, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sentence for input: Today we\n",
      "Today we should have spoke; for his father's sake, in honour of a true plantagenet, and rise resty grown; he bade me trust you; but i do not think i flatter; for what advancement may i think, is a smock, creaking my shoes on the plain words, by thy true-telling friend. and\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sentence('Today we', shakespeare_vocabulary, shakespeare_ngrams_grouped_by_prior_tokens, max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "Note: we didn't split training and test data, so perplexity is not really meaningful here, but this is the approach.\n",
    "\n",
    "First we load a 'test set' (in this case it matches the training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving token/words for ../Word2Vec/Alice.txt\n",
      "tokenised_document length: 29594\n"
     ]
    }
   ],
   "source": [
    "def load_tokenised_file(filename):\n",
    "    print('Retrieving token/words for ' + filename)\n",
    "    with open(filename, 'r', encoding=\"mbcs\") as file:\n",
    "        document = file.read()\n",
    "        tokenised_document = [token.casefold() for token in document.split()]\n",
    "        print(f'tokenised_document length: {len(tokenised_document)}')\n",
    "        return tokenised_document\n",
    "\n",
    "alice_test_set = load_tokenised_file('./datasets/Alice.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the log based perplexity calculation, as it the multiplication of many small probabilities will likely underflow the float, and as perplexity is a relative measure, the log based version is equally relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_based_perplexity(model, test_set):\n",
    "    '''Calculate the perplexity of a model on a training set.\n",
    "    \n",
    "    model: A function that generates the probabilities of the next token in a sequence.\n",
    "    test_set: A list of string tokens.\n",
    "    returns: The perplexity of the model on the training set.'''\n",
    "\n",
    "    N = len(test_set)\n",
    "    log_probabilities = [\n",
    "        (math.log(model(test_set[:i])[test_set[i]])) \n",
    "        for i in range(1, N)\n",
    "    ]\n",
    "    return math.exp(-sum(log_probabilities)/N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we calculate the perplexity of a random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5647.351155318548\n"
     ]
    }
   ],
   "source": [
    "def build_random_model(vocabulary):\n",
    "    '''Build a random model.\n",
    "    \n",
    "    vocabulary: A list of string tokens.\n",
    "    returns: A function that generates the next token in a sequence.'''\n",
    "\n",
    "    static_equal_probabilities = {token: 1/len(vocabulary) for token in vocabulary}\n",
    "    def random_model(_):\n",
    "        return static_equal_probabilities\n",
    "    return random_model\n",
    "\n",
    "random_model = build_random_model(set(alice_test_set))\n",
    "perplexity = calculate_log_based_perplexity(random_model, alice_test_set)\n",
    "print('Perplexity: ' + str(perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the ngram model has an excellent perplexity, as expected, as it is trained on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.0788138979888258\n"
     ]
    }
   ],
   "source": [
    "alice_model = lambda tokenised_input: get_vocabulary_probabilities(alice_vocabulary, tokenised_input, alice_ngrams_grouped_by_prior_tokens, max_n=10)\n",
    "perplexity = calculate_log_based_perplexity(alice_model, alice_test_set)\n",
    "print('Perplexity: ' + str(perplexity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
