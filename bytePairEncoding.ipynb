{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding Tokenization\n",
    "\n",
    "An algorithm used for tokenization of text, prior to analysis or processing. It is based on the idea of iteratively merging the most frequent pair of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Naive implementation: no word or sentence boundary detection, no special tokenization for numbers, dates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokens_using_byte_pair_encoding(document, vocabulary_size):\n",
    "    '''Builds a dictionary of tokens using byte pair encodings from the given document.\n",
    "\n",
    "    document: The document to build tokens from.\n",
    "    vocabulary_size: The maximum number of tokens to create.\n",
    "    returns: A dictionary of tokens and their frequencies\n",
    "    '''\n",
    "\n",
    "    tokenised_document = list(document)\n",
    "    tokens = set(tokenised_document)\n",
    "    \n",
    "    while len(tokens) < vocabulary_size:\n",
    "        token_pair = _find_most_frequent_token_pair(tokenised_document)\n",
    "        tokens.add(token_pair[0] + token_pair[1])\n",
    "        _merge_pairs(tokenised_document, token_pair)\n",
    "\n",
    "    return _get_token_frequencies(tokenised_document)\n",
    "\n",
    "def _tokenise(document, token_frequencies):\n",
    "    '''Tokenises a document using the given tokens in decreasing order of frequency.'''\n",
    "\n",
    "    tokenised_document = list(document)\n",
    "    sorted_tokens = sorted(token_frequencies.keys(), key=lambda token: token_frequencies[token], reverse=True)\n",
    "    for token in sorted_tokens:\n",
    "        tokenised_document = _merge_token_occurrences(tokenised_document, token)\n",
    "    return tokenised_document\n",
    "\n",
    "def _merge_pairs(tokenised_document, token_pair):\n",
    "    i = 0\n",
    "    while (i < len(tokenised_document)-1):\n",
    "        if (tokenised_document[i], tokenised_document[i+1]) == token_pair:\n",
    "            tokenised_document[i] = token_pair[0] + token_pair[1]\n",
    "            tokenised_document.pop(i+1)\n",
    "        i = i + 1\n",
    "\n",
    "def _find_most_frequent_token_pair(tokenised_document):\n",
    "    token_pairs = {}\n",
    "    last_token = tokenised_document[0]\n",
    "    for token in tokenised_document[1:]:\n",
    "        token_pair = (last_token, token)\n",
    "        token_pairs[token_pair] = token_pairs.get(token_pair, 0) + 1\n",
    "        last_token = token\n",
    "    most_frequent_token_pair = max(token_pairs, key=token_pairs.get)\n",
    "    return most_frequent_token_pair\n",
    "\n",
    "def _get_token_frequencies(tokenised_document):\n",
    "    token_frequencies = {}\n",
    "    for token in tokenised_document:\n",
    "        if token not in token_frequencies:\n",
    "            token_frequencies[token] = 0\n",
    "        token_frequencies[token] += 1\n",
    "    return token_frequencies\n",
    "\n",
    "def _merge_token_occurrences(tokenised_document, token):\n",
    "    token_length = len(token)\n",
    "    document_index = 0\n",
    "    while document_index < len(tokenised_document):\n",
    "        tokens_count_to_merge = 0\n",
    "        accumulated_token = tokenised_document[document_index]\n",
    "        while len(accumulated_token) < token_length and document_index+tokens_count_to_merge+1 < len(tokenised_document):\n",
    "            tokens_count_to_merge += 1\n",
    "            accumulated_token += tokenised_document[document_index+tokens_count_to_merge]\n",
    "        if accumulated_token == token:\n",
    "            tokenised_document = tokenised_document[:document_index] + [token] + tokenised_document[document_index+token_length:]\n",
    "        document_index += 1\n",
    "    return tokenised_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tokens for ../Word2Vec/Alice.txt\n",
      "Final tokens: {'The ': 72, 'Project Gutenberg': 21, ' ': 610, 'eB': 18, 'ook ': 42, 'of ': 193, 'Alice': 72, 'â€™s ': 96, 'A': 149, 'dv': 21, 'ent': 150, 'ure': 31, 's ': 359, 'in ': 139, 'W': 98, 'onder': 28, 'l': 281, 'and': 75, ', ': 470, 'by ': 30, 'L': 112, 'e': 520, 'w': 354, 'is ': 65, 'C': 104, 'ar': 152, 'ro': 68, 'll': 141, '\\n\\n': 71, 'T': 142, 'his ': 76, 'for the ': 19, 'use ': 35, 'of': 156, ' any': 24, 'one ': 99, 'an': 171, 'y': 344, 'here ': 28, 'in the ': 85, 'Un': 18, 'ited ': 20, 'St': 27, 'at': 200, 'es': 273, ' and': 64, '\\n': 730, 'mo': 38, 'st ': 60, 'other ': 28, 'part': 25, 'of the ': 133, 'wor': 25, 'd': 450, ' at ': 78, 'no ': 54, 'c': 277, 'o': 204, 'st': 199, ' and ': 169, 'with': 85, ' al': 36, 'r': 267, 'est': 45, 'ri': 126, 't': 502, 'ion': 100, 's\\n': 33, 'what': 26, 'so': 34, 'ever': 91, '. ': 205, 'You ': 24, 'may ': 21, 'cop': 21, 'y ': 240, 'it': 415, 'g': 294, 'ive ': 40, ' a': 166, 'way ': 41, 'or ': 108, 're': 114, '-': 151, 'it ': 253, 'under': 30, ' the ': 419, 'ter': 93, 'm': 381, 'ic': 126, 'en': 327, 'se ': 79, 'in': 389, 'cl': 60, 'ud': 27, 'ed ': 350, ' this ': 61, 'on': 301, 'e ': 366, 'ww': 10}\n",
      "Tokenised document['The ', 'j', 'ect ', 'ent', 're', 's ', 'in ', 'onder', 'by ', 'e', 'w', 'is ', 'ar', 'ro', 'll', '\\n\\n', 'T', 'his ', 'B', 'ook ', 'e ', 'of ', 'an', 'y', 'one ', 'w', 'her', 'in the ', 'at', 'es ', 'd', '\\n', 'mo', 'st ', 'th', 'er ', 'ar', 't', 's ', 'of the ', ' ', 'no', ' c', 'o', 'st', ' and ', 'h', ' al', 'st ', 'o ', 'r', 'est', 'c', 't', 'ion', '\\n', 'what', 'You', 'ay ', 'op', 'y ', 'it', ', ', 'g', 'ive ', 'e ', 'it ', 'n', 'der', 'ter', 's\\n', 'of the ', 'c', 't ', 'Gutenberg', 'en', 'se ', 'cl', 'ud', 'ed ', 'ith', 's ', 'eB', 'ook ', 'in', 'e ', 'at', '\\n', 'ww', 'w', '.', 'g', 'ut', 'en', 'ber', '.', 'or', 'g', '. I', 'you ']\n",
      "Calculating tokens for ../Word2Vec/Shakespear.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(tokens))\n\u001b[0;32m     15\u001b[0m calculate_and_print_tokens(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Word2Vec/Alice.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, vocabulary_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mcalculate_and_print_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../Word2Vec/Shakespear.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 8\u001b[0m, in \u001b[0;36mcalculate_and_print_tokens\u001b[1;34m(filename, vocabulary_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmbcs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      7\u001b[0m     document \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m----> 8\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_tokens_using_byte_pair_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal tokens: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(sample_dictionary(tokens, \u001b[38;5;241m100\u001b[39m)))\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenised document\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(tokenise(document, tokens)[:\u001b[38;5;241m100\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[43], line 15\u001b[0m, in \u001b[0;36mbuild_tokens_using_byte_pair_encoding\u001b[1;34m(document, vocabulary_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m     token_pair \u001b[38;5;241m=\u001b[39m find_most_frequent_token_pair(tokenised_document)\n\u001b[0;32m     14\u001b[0m     tokens\u001b[38;5;241m.\u001b[39madd(token_pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m token_pair[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mmerge_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenised_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_pair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_token_frequencies(tokenised_document)\n",
      "Cell \u001b[1;32mIn[43], line 30\u001b[0m, in \u001b[0;36mmerge_pairs\u001b[1;34m(tokenised_document, token_pair)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_pairs\u001b[39m(tokenised_document, token_pair):\n\u001b[0;32m     29\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenised_document\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (tokenised_document[i], tokenised_document[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m token_pair:\n\u001b[0;32m     32\u001b[0m             tokenised_document[i] \u001b[38;5;241m=\u001b[39m token_pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m token_pair[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def calculate_and_print_tokens(filename, vocabulary_size):\n",
    "    print('Calculating tokens for ' + filename)\n",
    "    with open(filename, 'r', encoding=\"mbcs\") as file:\n",
    "        document = file.read()\n",
    "        tokens = build_tokens_using_byte_pair_encoding(document, vocabulary_size)\n",
    "        print('Final tokens: ' + str(_sample_dictionary(tokens, 100)))\n",
    "        with open(filename + '.' + str(vocabulary_size) + '.tokens', 'w') as file:\n",
    "            file.write(str(tokens))\n",
    "        print('Tokenised document' + str(_tokenise(document, tokens)[:100]))\n",
    "\n",
    "def _sample_dictionary(dictionary, sample_size):\n",
    "    return {key: value for index, (key, value) in enumerate(dictionary.items()) if index < sample_size}\n",
    "    \n",
    "calculate_and_print_tokens('./datasets/Alice.txt', vocabulary_size=1000)\n",
    "calculate_and_print_tokens('./datasets/Shakespeare.txt', vocabulary_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
