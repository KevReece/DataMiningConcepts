{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heaps' Law\n",
    "\n",
    "This law describes the relationship between the number of distinct words in a document and the number of words in the document. It is given by the equation:\n",
    "\n",
    "$$\n",
    "V = KN^\\beta\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $V$ is the number of distinct words in a document\n",
    "- $N$ is the number of words in the document\n",
    "- $K$ and $\\beta$ are free parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_HEAPS_K = 10\n",
    "LOW_HEAPS_BETA = 0.4\n",
    "HIGH_HEAPS_K = 100\n",
    "HIGH_HEAPS_BETA = 0.6\n",
    "\n",
    "def linear_midpoint(lowpoint, highpoint):\n",
    "    return lowpoint + (highpoint - lowpoint) * 0.5\n",
    "\n",
    "def exponential_midpoint(lowpoint, highpoint):\n",
    "    return lowpoint * (highpoint / lowpoint) ** 0.5\n",
    "\n",
    "TYPICAL_HEAPS_K = exponential_midpoint(LOW_HEAPS_K, HIGH_HEAPS_K)\n",
    "TYPICAL_HEAPS_BETA = linear_midpoint(LOW_HEAPS_BETA, HIGH_HEAPS_BETA)\n",
    "\n",
    "def calculate_heaps_law_size(text_size, k=TYPICAL_HEAPS_K, beta=TYPICAL_HEAPS_BETA):\n",
    "    '''Calculates the number of distinct vocabulary words for a given number text using Heaps' law.\n",
    "    \n",
    "    text_size: int, the text size\n",
    "    k: float, the k parameter of Heaps' law\n",
    "    beta: float, the beta parameter of Heaps' law\n",
    "    return: float, the predicted number of distinct vocabulary words\n",
    "    '''\n",
    "    return k * (text_size ** beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../Word2Vec/Alice.txt: \n",
      "Document size: 170548\n",
      "Distinct words: 5980\n",
      "Low Heaps' law size: 1238.0513336871213\n",
      "Typical Heaps' law size: 13059.402742851606\n",
      "High Heaps' law size: 137755.1926640068\n",
      "\n",
      "../Word2Vec/Shakespear.txt: \n",
      "Document size: 5458198\n",
      "Distinct words: 67505\n",
      "Low Heaps' law size: 4952.445607328613\n",
      "Typical Heaps' law size: 73879.61829895985\n",
      "High Heaps' law size: 1102121.746056731\n"
     ]
    }
   ],
   "source": [
    "def calculate_and_print_document_sizes(filename):\n",
    "    with open(filename, 'r', encoding=\"mbcs\") as file:\n",
    "        document = file.read()\n",
    "        document_size = len(document)\n",
    "        print_document_sizes(filename, document, document_size)\n",
    "\n",
    "def print_document_sizes(filename, document, document_size):\n",
    "    print('\\n' + filename + ': ')\n",
    "    print('Document size: ' + str(document_size))\n",
    "    print('Distinct words: ' + str(len(set(document.split()))))\n",
    "    print('Low Heaps\\' law size: ' + str(calculate_heaps_law_size(document_size, LOW_HEAPS_K, LOW_HEAPS_BETA)))\n",
    "    print('Typical Heaps\\' law size: ' + str(calculate_heaps_law_size(document_size)))\n",
    "    print('High Heaps\\' law size: ' + str(calculate_heaps_law_size(document_size, HIGH_HEAPS_K, HIGH_HEAPS_BETA)))\n",
    "\n",
    "calculate_and_print_document_sizes('./datasets/Alice.txt')\n",
    "calculate_and_print_document_sizes('./datasets/Shakespeare.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
