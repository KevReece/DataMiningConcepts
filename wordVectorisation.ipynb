{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorisation\n",
    "\n",
    "Below are multiple techniques to convert words into vectors. \n",
    "\n",
    "Note: tokenisation here is a simple word based split and clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 29521\n",
      "Vocabulary size: 4275\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(filename):\n",
    "    '''Load a corpus from a file and return a list of tokens and a list of documents.\n",
    "    \n",
    "    filename: the name of the file to load\n",
    "    returns: a tuple containing a list of tokens and a list of documents'''\n",
    "    \n",
    "    corpus = []\n",
    "    corpus_documents = []\n",
    "    next_document = []\n",
    "    with open(filename, 'r', encoding='mbcs') as file:\n",
    "        for line in file:\n",
    "            line_tokens = line.strip().split()\n",
    "            cleaned_line_tokens = [\n",
    "                re.sub(r'^\\W+|\\W+$', '', token.casefold()) \n",
    "                for token in line_tokens ]\n",
    "            cleaned_line_tokens = [token for token in cleaned_line_tokens if token != '']\n",
    "            corpus.extend(cleaned_line_tokens)\n",
    "            next_document.extend(cleaned_line_tokens)\n",
    "            if len(cleaned_line_tokens) == 0 and len(next_document) > 0:\n",
    "                corpus_documents.append(next_document)\n",
    "                next_document = []\n",
    "    if len(next_document) > 0:\n",
    "        corpus_documents.append(next_document)\n",
    "\n",
    "    vocabulary = list(set(corpus))\n",
    "    return corpus, vocabulary, corpus_documents\n",
    "\n",
    "corpus, vocabulary, corpus_documents = load_corpus('./datasets/Alice.txt')\n",
    "\n",
    "print('Corpus length:', len(corpus))\n",
    "print('Vocabulary size:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of vectors\n",
    "\n",
    "We simply display the vector similarities for visual validation as evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    '''Compute the cosine similarity between two vectors.\n",
    "\n",
    "    vector1: the first vector\n",
    "    vector2: the second vector\n",
    "    returns: the cosine similarity between the two vectors'''\n",
    "\n",
    "    dot_product = sum([v1_dim*v2_dim for v1_dim, v2_dim in zip(vector1, vector2)])\n",
    "    v1_magnitude = sum([v1_dim**2 for v1_dim in vector1])**0.5\n",
    "    v2_magnitude = sum([v2_dim**2 for v2_dim in vector2])**0.5\n",
    "    return dot_product / (v1_magnitude * v2_magnitude)\n",
    "\n",
    "def print_similarities(vectors, tokens=[\"tea\", \"cakes\", \"tarts\", \"dormouse\"]):\n",
    "    '''Print the cosine similarity between pairs of tokens.\n",
    "\n",
    "    vectors: a dictionary of word vectors\n",
    "    tokens: a list of tokens to compare'''\n",
    "\n",
    "    for index, token1 in enumerate(tokens[:-1]):\n",
    "        for token2 in tokens[index+1:]:\n",
    "            similarity = cosine_similarity(vectors[token1], vectors[token2])\n",
    "            print(f'\\'{token1}\\' and \\'{token2}\\' similarity = {similarity:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand Crafted Vectors to demonstrate evaluation\n",
    "\n",
    "Dimensions:\n",
    "- potable\n",
    "- edible\n",
    "- animated-ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tea' and 'cakes' similarity = 0.5545\n",
      "'tea' and 'tarts' similarity = 0.5621\n",
      "'tea' and 'dormouse' similarity = 0.2171\n",
      "'cakes' and 'tarts' similarity = 1.0000\n",
      "'cakes' and 'dormouse' similarity = 0.2171\n",
      "'tarts' and 'dormouse' similarity = 0.2175\n"
     ]
    }
   ],
   "source": [
    "hand_crafted_vectors = {\n",
    "    \"tea\":      [1,     0.3,    0.1],\n",
    "    \"cakes\":    [0.3,   1,      0.1],\n",
    "    \"tarts\":    [0.31,  1,      0.1],\n",
    "    \"dormouse\": [0.1,   0.1,    1]\n",
    "}\n",
    "\n",
    "print_similarities(hand_crafted_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to word vector (colocation)\n",
    "\n",
    "Based on a training set, look at all of the words that appear in the same context as the target word (i.e. X nearest words left/right). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tea' and 'cakes' similarity = 0.0769\n",
      "'tea' and 'tarts' similarity = 0.0769\n",
      "'tea' and 'dormouse' similarity = 0.3947\n",
      "'cakes' and 'tarts' similarity = 0.2308\n",
      "'cakes' and 'dormouse' similarity = 0.1316\n",
      "'tarts' and 'dormouse' similarity = 0.2193\n"
     ]
    }
   ],
   "source": [
    "def calculate_word_to_word_vectors(vocabulary, corpus, context_size=10):\n",
    "    '''Calculate the word-to-word vectors for a corpus.\n",
    "\n",
    "    vocabulary: the list of words in the vocabulary\n",
    "    corpus: the list of words in the corpus\n",
    "    context_size: the number of words to consider before and after the target word\n",
    "    returns: a dictionary of word vectors'''\n",
    "    \n",
    "    word_to_word_vectors = {}\n",
    "    for word in vocabulary:\n",
    "        word_to_word_vectors[word] = [0] * len(vocabulary)\n",
    "    vocabulary_indices = {word: index for index, word in enumerate(vocabulary)}\n",
    "    for corpus_index, word in enumerate(vocabulary):\n",
    "        context_start = max(0, corpus_index - context_size)\n",
    "        context_end = min(len(corpus), corpus_index + context_size + 1)\n",
    "        for word_context_index in range(context_start, context_end):\n",
    "            if corpus_index == word_context_index: \n",
    "                continue\n",
    "            context_word = corpus[word_context_index]\n",
    "            word_context_vocabulary_index = vocabulary_indices[context_word]\n",
    "            word_to_word_vectors[word][word_context_vocabulary_index] += 1\n",
    "    return word_to_word_vectors\n",
    "\n",
    "word_to_word_vectors = calculate_word_to_word_vectors(vocabulary, corpus)\n",
    "\n",
    "print_similarities(word_to_word_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Term Frequency - Inverse Document Frequency. This is used to account for the natural frequency of words in documents. This is based on a corpus documents, and ratio of documents that contain the word, as a divisor of vector word frequency.\n",
    "\n",
    "Note: in this case we have one text file as the corpora, so have split by blank line separated paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tea' and 'cakes' similarity = 0.0745\n",
      "'tea' and 'tarts' similarity = 0.0736\n",
      "'tea' and 'dormouse' similarity = 0.4137\n",
      "'cakes' and 'tarts' similarity = 0.1491\n",
      "'cakes' and 'dormouse' similarity = 0.0976\n",
      "'tarts' and 'dormouse' similarity = 0.2009\n"
     ]
    }
   ],
   "source": [
    "def tfidf_normalise_word_to_word_vectors(word_to_word_vectors, corpus_documents):\n",
    "    '''Normalise word-to-word vectors using TF-IDF.\n",
    "\n",
    "    word_to_word_vectors: a dictionary of word vectors\n",
    "    corpus_documents: a list of documents in the corpus\n",
    "    returns: a dictionary of TF-IDF normalised word vectors'''\n",
    "    \n",
    "    log_word_to_word_vectors = {\n",
    "        word: [math.log(frequency + 1, 10) for frequency in vector] \n",
    "        for word, vector in word_to_word_vectors.items()}\n",
    "    document_frequencies = {word: 0 for word in word_to_word_vectors}\n",
    "    for document in corpus_documents:\n",
    "        for word in set(document):\n",
    "            document_frequencies[word] += 1\n",
    "    idf_word_to_word_vectors = {\n",
    "        word: math.log(len(corpus_documents) / document_frequencies[word], 10) \n",
    "        for word in word_to_word_vectors.keys()}\n",
    "    tfidf_word_to_word_vectors = {\n",
    "        word: [\n",
    "            log_frequency * idf_word_to_word_vectors[word] \n",
    "            for log_frequency in vector] \n",
    "        for word, vector in log_word_to_word_vectors.items()}\n",
    "    return tfidf_word_to_word_vectors\n",
    "\n",
    "tfidf_word_to_word_vectors = tfidf_normalise_word_to_word_vectors(word_to_word_vectors, corpus_documents)\n",
    "\n",
    "print_similarities(tfidf_word_to_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI\n",
    "\n",
    "Pointwise Mutual Information. This is a measure of how much more likely the words are to appear together than if they were independent.\n",
    "\n",
    "Note: the following implementation includes a add-one smoothing, which is advised to remove bias to infrequent words, but also avoids a log of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tea' and 'cakes' similarity = 0.9997\n",
      "'tea' and 'tarts' similarity = 1.0000\n",
      "'tea' and 'dormouse' similarity = 0.9994\n",
      "'cakes' and 'tarts' similarity = 0.9997\n",
      "'cakes' and 'dormouse' similarity = 0.9983\n",
      "'tarts' and 'dormouse' similarity = 0.9993\n"
     ]
    }
   ],
   "source": [
    "def pmi_normalise_word_to_word_vectors(word_to_word_vectors, corpus, vocabulary):\n",
    "    '''Normalise word-to-word vectors using PMI.\n",
    "\n",
    "    word_to_word_vectors: a dictionary of word vectors\n",
    "    corpus: a list of words in the corpus\n",
    "    vocabulary: a list of words in the vocabulary\n",
    "    returns: a dictionary of PMI normalised word vectors'''\n",
    "    \n",
    "    word_frequency = {word: 0 for word in word_to_word_vectors}\n",
    "    for word in corpus:\n",
    "        word_frequency[word] += 1\n",
    "    corpus_size = len(corpus)\n",
    "    pmi_word_to_word_vectors = {\n",
    "        word: [\n",
    "            max(0, math.log((corpus_size * (frequency + 1)) / (word_frequency[word] * word_frequency[vocabulary[context_word_index]]), 10)) \n",
    "            for context_word_index, frequency in enumerate(vector)] \n",
    "        for word, vector in word_to_word_vectors.items()}\n",
    "    return pmi_word_to_word_vectors\n",
    "\n",
    "pmi_word_to_word_vectors = pmi_normalise_word_to_word_vectors(word_to_word_vectors, corpus, vocabulary)\n",
    "\n",
    "print_similarities(pmi_word_to_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And for Shakespeare\n",
    "\n",
    "Note: this takes 40 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 900989\n",
      "Vocabulary size: 29117\n",
      "'wherefore' and 'art' similarity = 0.0833\n",
      "'wherefore' and 'macbeth' similarity = 0.0772\n",
      "'wherefore' and 'dagger' similarity = 0.1250\n",
      "'wherefore' and 'poison' similarity = 0.0870\n",
      "'art' and 'macbeth' similarity = 0.3858\n",
      "'art' and 'dagger' similarity = 0.2083\n",
      "'art' and 'poison' similarity = 0.0870\n",
      "'macbeth' and 'dagger' similarity = 0.1157\n",
      "'macbeth' and 'poison' similarity = 0.1612\n",
      "'dagger' and 'poison' similarity = 0.0435\n",
      "'wherefore' and 'art' similarity = 0.9983\n",
      "'wherefore' and 'macbeth' similarity = 0.9998\n",
      "'wherefore' and 'dagger' similarity = 0.9997\n",
      "'wherefore' and 'poison' similarity = 0.9999\n",
      "'art' and 'macbeth' similarity = 0.9992\n",
      "'art' and 'dagger' similarity = 0.9966\n",
      "'art' and 'poison' similarity = 0.9975\n",
      "'macbeth' and 'dagger' similarity = 0.9991\n",
      "'macbeth' and 'poison' similarity = 0.9995\n",
      "'dagger' and 'poison' similarity = 0.9999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shakespeare_corpus, shakespeare_vocabulary, shakespeare_corpus_documents = load_corpus('../datasets/Shakespeare.txt')\n",
    "\n",
    "print('Corpus length:', len(shakespeare_corpus))\n",
    "print('Vocabulary size:', len(shakespeare_vocabulary))\n",
    "\n",
    "shakespeare_word_to_word_vectors = calculate_word_to_word_vectors(shakespeare_vocabulary, shakespeare_corpus)\n",
    "\n",
    "print_similarities(shakespeare_word_to_word_vectors, tokens=[\"wherefore\", \"art\", \"macbeth\", \"dagger\", \"poison\"])\n",
    "\n",
    "shakespeare_pmi_word_to_word_vectors = pmi_normalise_word_to_word_vectors(shakespeare_word_to_word_vectors, shakespeare_corpus, shakespeare_vocabulary)\n",
    "\n",
    "print_similarities(shakespeare_pmi_word_to_word_vectors, tokens=[\"wherefore\", \"art\", \"macbeth\", \"dagger\", \"poison\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tea' and 'cakes' similarity = 0.9692\n",
      "'tea' and 'tarts' similarity = 0.9937\n",
      "'tea' and 'dormouse' similarity = 0.9971\n",
      "'cakes' and 'tarts' similarity = 0.9576\n",
      "'cakes' and 'dormouse' similarity = 0.9663\n",
      "'tarts' and 'dormouse' similarity = 0.9972\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "model = Word2Vec(corpus_documents, vector_size=100, min_count=1, window=10)\n",
    "model.train(corpus_documents, total_examples=len(corpus_documents), epochs=10)\n",
    "\n",
    "vectors = {word: model.wv[word] for word in model.wv.key_to_index.keys()}\n",
    "\n",
    "print_similarities(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec most_similiar()\n",
    "\n",
    "This is a useful function of the library, based on the most similar vectors to the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thought', 0.9988740682601929), ('herself', 0.9986680746078491), ('much', 0.99854975938797), ('very', 0.998485267162323), ('poor', 0.9984777569770813)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=[\"alice\"], topn=5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
